{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow as tf2\n",
    "\n",
    "check_data = h5py.File('indoor_flying_preprocessed.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.keras.backend.set_floatx('float32') #for memory use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_height = 260\n",
    "target_width = 340\n",
    "\n",
    "#shuffle data \n",
    "from random import shuffle\n",
    "ind_list = [i for i in range(len(check_data['event_images']))]\n",
    "shuffle(ind_list)\n",
    "event_images = [check_data['event_images'][i] for i in ind_list]\n",
    "depth_images = [check_data['depth_images_timed'][i] for i in ind_list]\n",
    "\n",
    "\n",
    "def get_batches_fn(batch_size):\n",
    "    # shuffle data\n",
    "    ind_list = [i for i in range(len(check_data['event_images']))]\n",
    "    shuffle(ind_list)\n",
    "    event_images = [check_data['event_images'][i] for i in ind_list]\n",
    "    depth_images = [check_data['depth_images_timed'][i] for i in ind_list]\n",
    "    for batch_i in range(0, len(event_images), batch_size):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for i in range(batch_i, batch_i+batch_size):\n",
    "            inputs.append(event_images[i][0:target_height, 0:target_width, :])\n",
    "            output_image = depth_images[i][0:target_height, 0:target_width].reshape((target_height, target_width, 1))\n",
    "            output_image[np.isnan(output_image)] = 0.0\n",
    "            outputs.append(output_image)\n",
    "\n",
    "            cost_weights = np.copy(outputs)\n",
    "            cost_weights[cost_weights>0] = 1.0\n",
    "\n",
    "        yield np.array(inputs), np.array(outputs), np.array(cost_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(260, 340, 3)\n"
    }
   ],
   "source": [
    "print(check_data['event_images'][10][0:target_height, 0:target_width, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 []>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(input_tensor):\n",
    "    \n",
    "    #Layer 1: Convolutional\n",
    "    conv_1 = tf.layers.Conv2D(kernel_size=(10, 10), filters=64, strides=1, padding='same', \n",
    "                kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                kernel_regularizer=tf2.keras.regularizers.l2(), bias_regularizer=tf2.keras.regularizers.l2(),\n",
    "                name='conv_1')(input_tensor)\n",
    "\n",
    "    #Layer 2: Convolutional\n",
    "    conv_2 = tf.layers.Conv2D(kernel_size=10, filters=64, strides=1, padding='same', \n",
    "                kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                kernel_regularizer=tf2.keras.regularizers.l2(), bias_regularizer=tf2.keras.regularizers.l2(), activation='relu',\n",
    "                name='conv_2')(conv_1)\n",
    "    \n",
    "\n",
    "    #pooling function\n",
    "    pool_1 = tf.layers.MaxPooling2D(pool_size=2, strides=2, padding='same', name='pool_1')(conv_2)\n",
    "\n",
    "    #Layer 3: Convolutional\n",
    "    conv_3 = tf.layers.Conv2D(kernel_size=10, filters=128, strides=1, padding='same', \n",
    "                kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                kernel_regularizer=tf2.keras.regularizers.l2(), bias_regularizer=tf2.keras.regularizers.l2(),\n",
    "                name='conv_3')(pool_1)\n",
    "\n",
    "    #Layer 4: Convolutional\n",
    "    conv_4 = tf.layers.Conv2D(kernel_size=10, filters=128, strides=1, padding='same', \n",
    "                kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                kernel_regularizer=tf2.keras.regularizers.l2(), bias_regularizer=tf2.keras.regularizers.l2(), activation='relu',\n",
    "                name='conv_4')(conv_3)\n",
    "\n",
    "    #pooling function\n",
    "    pool_2 = tf.layers.MaxPooling2D(pool_size=2, strides=2, padding='same', name='pool_2')(conv_4)\n",
    "\n",
    "\n",
    "    #1x1 convolutions 1\n",
    "    conv1x1_1 = tf.layers.Conv2D(kernel_size=1, filters=256, strides=1, padding='same',\n",
    "                    kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                    kernel_regularizer=tf2.keras.regularizers.l2(), \n",
    "                    name='conv1x1_1')(pool_2)\n",
    "    \n",
    "    tanh_1 = tf2.keras.activations.tanh(conv1x1_1)\n",
    "\n",
    "    #1x1 convolutions 2\n",
    "    conv1x1_2 = tf.layers.Conv2D(kernel_size=1, filters=512, strides=1, padding='same',\n",
    "                    kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                    kernel_regularizer=tf2.keras.regularizers.l2(),\n",
    "                    name='conv1x1_2')(tanh_1)\n",
    "\n",
    "    \n",
    "    tanh_2 = tf2.keras.activations.tanh(conv1x1_2)\n",
    "\n",
    "\n",
    "    #Deconvolution 1\n",
    "    deconv_1 = tf2.keras.layers.Conv2DTranspose(kernel_size=10, filters=128, strides=2, padding='same',\n",
    "                    kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                    kernel_regularizer=tf2.keras.regularizers.l2(),  activation='relu',\n",
    "                    name='deconv_1')(tanh_2)\n",
    "                    \n",
    "    #Skipping layer 1\n",
    "    skip_1 = tf2.keras.layers.Add(name='skip_1')([deconv_1, conv_4])\n",
    "\n",
    "    #Deconvolution 1\n",
    "    deconv_2 = tf2.keras.layers.Conv2DTranspose(kernel_size=10, filters=64, strides=2, padding='same',\n",
    "                    kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                    kernel_regularizer=tf2.keras.regularizers.l2(), activation='relu',\n",
    "                    name='deconv_2')(skip_1)\n",
    "    \n",
    "    #Skipping layer 2\n",
    "    skip_2 = tf2.keras.layers.Add(name='skip_2')([deconv_2, conv_2])\n",
    "\n",
    "    #Final layer\n",
    "    output_layer = tf.layers.Conv2D(kernel_size=10, filters=1, strides=1, padding='same',\n",
    "                    kernel_initializer=tf2.keras.initializers.GlorotNormal(), bias_initializer=tf2.keras.initializers.GlorotNormal(),\n",
    "                    kernel_regularizer=tf2.keras.regularizers.l2(), \n",
    "                    name='output_layer')(skip_2)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_value, cost_weights, learning_rate):\n",
    "    \"\"\"\n",
    "    Build the TensorFLow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label \n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"    \n",
    "    #reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) #regularization loss\n",
    "    #mse_loss = tf.keras.losses.MeanSquaredError()(tf2.math.multiply(correct_value, correct_value), tf2.math.multiply(nn_last_layer, correct_value)) #cross entropy\n",
    "    #overall loss combining cross entropy and regularization\n",
    "    #overall_loss = tf2.math.add(1.0*tf2.reduce_sum(reg_loss), mse_loss, name='total_loss')\n",
    "\n",
    "    mse_loss = tf.keras.losses.MeanSquaredError()(correct_value, tf2.math.multiply(nn_last_layer, cost_weights)) #cross entropy\n",
    "  \n",
    "    #obtain training operation\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, epsilon = 0.000001) #Note default value of epsilon 1e-8 results in instability after few epochs\n",
    "   \n",
    "    #clip the gradients\n",
    "    gvs = optimizer.compute_gradients(mse_loss)\n",
    "    #capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "    training_operation = optimizer.apply_gradients(gvs)\n",
    "\n",
    "    return nn_last_layer, training_operation, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, loss_function, input_image,\n",
    "             depth_image, costs_weights, learning_rate, base_learning_rate):\n",
    "    \"\"\"\n",
    "    Train neural network and print    out the loss during training.\n",
    "  param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param output_image: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    #initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    scaling_rate = 1\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        if i > 1:\n",
    "          scaling_rate = 0.5 * scaling_rate\n",
    "        for image, output_image, costs in get_batches_fn(batch_size):\n",
    "\n",
    "            optimizer, loss = sess.run([train_op, loss_function], \n",
    "                               feed_dict={input_image: image, depth_image: output_image, costs_weights: costs, learning_rate: scaling_rate*base_learning_rate})\n",
    "            print(\"Loss: =\")\n",
    "            print(loss)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nTraining...\n\nEPOCH 1 ...\nLoss: =\n3.8189669\n\nLoss: =\n5.6658134\n\nLoss: =\n4.9568863\n\nLoss: =\n3.181001\n\nLoss: =\n3.6343286\n\nLoss: =\n2.8817313\n\nLoss: =\n2.4874167\n\nLoss: =\n1.9631627\n\nLoss: =\n2.598578\n\nLoss: =\n2.8947523\n\nLoss: =\n2.286173\n\nLoss: =\n2.3715034\n\nLoss: =\n1.8515717\n\nLoss: =\n2.384304\n\nLoss: =\n2.1634274\n\nLoss: =\n1.9007412\n\nLoss: =\n1.5976202\n\nLoss: =\n1.641837\n\nLoss: =\n1.7707744\n\nLoss: =\n1.168307\n\nEPOCH 2 ...\nLoss: =\n1.527378\n\nLoss: =\n1.5142325\n\nLoss: =\n1.558737\n\nLoss: =\n1.1203138\n\nLoss: =\n1.1809367\n\nLoss: =\n1.1682237\n\nLoss: =\n1.2975161\n\nLoss: =\n1.4945607\n\nLoss: =\n1.1636338\n\nLoss: =\n1.9034463\n\nLoss: =\n1.2491916\n\nLoss: =\n1.3484462\n\nLoss: =\n1.1430475\n\nLoss: =\n1.3737319\n\nLoss: =\n1.398198\n\nLoss: =\n1.6138952\n\nLoss: =\n1.2676265\n\nLoss: =\n1.2895448\n\nLoss: =\n1.394231\n\nLoss: =\n1.4365782\n\nEPOCH 3 ...\nLoss: =\n1.3877321\n\nLoss: =\n1.391662\n\nLoss: =\n1.2734365\n\nLoss: =\n1.2197832\n\nLoss: =\n1.4701116\n\nLoss: =\n1.2070272\n\nLoss: =\n1.4401618\n\nLoss: =\n1.0806435\n\nLoss: =\n1.1075709\n\nLoss: =\n1.2428193\n\nLoss: =\n1.4002427\n\nLoss: =\n1.3854587\n\nLoss: =\n1.3385664\n\nLoss: =\n1.4816946\n\nLoss: =\n1.3186349\n\nLoss: =\n1.1538241\n\nLoss: =\n1.1956164\n\nLoss: =\n1.3320003\n\nLoss: =\n1.051192\n\nLoss: =\n1.2189248\n\nEPOCH 4 ...\nLoss: =\n1.2629341\n\nLoss: =\n1.186076\n\nLoss: =\n1.1599876\n\nLoss: =\n1.2102431\n\nLoss: =\n1.4243497\n\nLoss: =\n1.0498405\n\nLoss: =\n1.1978258\n\nLoss: =\n1.3914005\n\nLoss: =\n1.0594002\n\nLoss: =\n1.1787428\n\nLoss: =\n0.9867744\n\nLoss: =\n1.3266498\n\nLoss: =\n1.4128207\n\nLoss: =\n1.3462961\n\nLoss: =\n1.5046678\n\nLoss: =\n1.3448766\n\nLoss: =\n1.1685021\n\nLoss: =\n1.4385724\n\nLoss: =\n1.3158\n\nLoss: =\n1.1720742\n\nEPOCH 5 ...\nLoss: =\n0.89121145\n\nLoss: =\n1.1543083\n\nLoss: =\n1.2073386\n\nLoss: =\n1.322289\n\nLoss: =\n1.2903361\n\nLoss: =\n1.0646\n\nLoss: =\n1.2977849\n\nLoss: =\n1.1020142\n\nLoss: =\n1.3181796\n\nLoss: =\n1.0737842\n\nLoss: =\n1.1621099\n\nLoss: =\n1.4699293\n\nLoss: =\n1.3599768\n\nLoss: =\n1.4389113\n\nLoss: =\n1.1555104\n\nLoss: =\n1.0386811\n\nLoss: =\n1.5029978\n\nLoss: =\n1.4574932\n\nLoss: =\n1.253364\n\nLoss: =\n1.2317353\n\nEPOCH 6 ...\nLoss: =\n1.6124657\n\nLoss: =\n1.1864522\n\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a93d2a5c48e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Train NN using the train_nn function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m train_nn(sess, epochs, batch_size, get_batches_fn, train_op, mse_loss, input_image,\n\u001b[1;32m---> 21\u001b[1;33m         depth_image, costs_weights, learning_rate, base_learning_rate)\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# save trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-9768261babd7>\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(sess, epochs, batch_size, get_batches_fn, train_op, loss_function, input_image, depth_image, costs_weights, learning_rate, base_learning_rate)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             optimizer, loss = sess.run([train_op, loss_function], \n\u001b[1;32m---> 30\u001b[1;33m                                feed_dict={input_image: image, depth_image: output_image, costs_weights: costs, learning_rate: scaling_rate*base_learning_rate})\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: =\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 960\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    961\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1183\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1361\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1350\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1352\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1443\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1444\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1445\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "batch_size = 5\n",
    "base_learning_rate = 0.00001\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "input_image = tf.placeholder(tf.float32, [None, None, None, 3], name='input_image')\n",
    "depth_image = tf.placeholder(tf.float32, [None, None, None, 1], name='depth_image')\n",
    "costs_weights = tf.placeholder(tf.float32, [None, None, None, 1], name='cost_weights')\n",
    "learning_rate = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Build NN using load_vgg, layers, and optimize function\n",
    "nn_last_layer = NN_model(input_image)\n",
    "\n",
    "logits, train_op, mse_loss = optimize(nn_last_layer, depth_image, costs_weights, learning_rate)\n",
    "\n",
    "# Train NN using the train_nn function\n",
    "train_nn(sess, epochs, batch_size, get_batches_fn, train_op, mse_loss, input_image,\n",
    "        depth_image, costs_weights, learning_rate, base_learning_rate)\n",
    "        \n",
    "# save trained model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FailedPreconditionError",
     "evalue": "2 root error(s) found.\n  (0) Failed precondition: Error while reading resource variable output_layer/kernel/Adam_1 from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/output_layer/kernel/Adam_1)\n\t [[node output_layer/kernel/Adam_1/Read/ReadVariableOp (defined at <ipython-input-8-37efa903502c>:21) ]]\n\t [[conv_2_1/bias/Adam_1/Read/ReadVariableOp/_97]]\n  (1) Failed precondition: Error while reading resource variable output_layer/kernel/Adam_1 from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/output_layer/kernel/Adam_1)\n\t [[node output_layer/kernel/Adam_1/Read/ReadVariableOp (defined at <ipython-input-8-37efa903502c>:21) ]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'output_layer/kernel/Adam_1/Read/ReadVariableOp':\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-a93d2a5c48e7>\", line 17, in <module>\n    logits, train_op, mse_loss = optimize(nn_last_layer, depth_image, costs_weights, learning_rate)\n  File \"<ipython-input-8-37efa903502c>\", line 21, in optimize\n    training_operation = optimizer.apply_gradients(gvs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\", line 597, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\adam.py\", line 132, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\", line 1156, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py\", line 197, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py\", line 171, in create_slot_with_initializer\n    dtype)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py\", line 73, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 1572, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 1315, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 568, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 520, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 938, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2596, in default_variable_creator\n    shape=shape)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1411, in __init__\n    distribute_strategy=distribute_strategy)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1592, in _init_from_args\n    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\", line 483, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3322, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1756, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1351\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1352\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1444\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1445\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Error while reading resource variable output_layer/kernel/Adam_1 from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/output_layer/kernel/Adam_1)\n\t [[{{node output_layer/kernel/Adam_1/Read/ReadVariableOp}}]]\n\t [[conv_2_1/bias/Adam_1/Read/ReadVariableOp/_97]]\n  (1) Failed precondition: Error while reading resource variable output_layer/kernel/Adam_1 from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/output_layer/kernel/Adam_1)\n\t [[{{node output_layer/kernel/Adam_1/Read/ReadVariableOp}}]]\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8b55b996212f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[0;32m   1191\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1192\u001b[0m                   save_path))\n\u001b[1;32m-> 1193\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[0;32m   1174\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[0;32m   1175\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 960\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    961\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1183\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1361\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1384\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1386\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Error while reading resource variable output_layer/kernel/Adam_1 from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/output_layer/kernel/Adam_1)\n\t [[node output_layer/kernel/Adam_1/Read/ReadVariableOp (defined at <ipython-input-8-37efa903502c>:21) ]]\n\t [[conv_2_1/bias/Adam_1/Read/ReadVariableOp/_97]]\n  (1) Failed precondition: Error while reading resource variable output_layer/kernel/Adam_1 from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/output_layer/kernel/Adam_1)\n\t [[node output_layer/kernel/Adam_1/Read/ReadVariableOp (defined at <ipython-input-8-37efa903502c>:21) ]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'output_layer/kernel/Adam_1/Read/ReadVariableOp':\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-a93d2a5c48e7>\", line 17, in <module>\n    logits, train_op, mse_loss = optimize(nn_last_layer, depth_image, costs_weights, learning_rate)\n  File \"<ipython-input-8-37efa903502c>\", line 21, in optimize\n    training_operation = optimizer.apply_gradients(gvs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\", line 597, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\adam.py\", line 132, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\", line 1156, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py\", line 197, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py\", line 171, in create_slot_with_initializer\n    dtype)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py\", line 73, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 1572, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 1315, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 568, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 520, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 938, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2596, in default_variable_creator\n    shape=shape)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1411, in __init__\n    distribute_strategy=distribute_strategy)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1592, in _init_from_args\n    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\", line 483, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3322, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1756, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'check_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-f833680fd55b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'depth_images_timed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_width\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'check_data' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(check_data['depth_images_timed'][65][0:target_height, 0:target_width], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'check_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-651b6061aabf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'event_images'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mest_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnn_last_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msample_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m260\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m340\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest_depth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m260\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m340\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'check_data' is not defined"
     ]
    }
   ],
   "source": [
    "sample_image = check_data['event_images'][65][0:target_height, 0:target_width, :]\n",
    "est_depth = sess.run([nn_last_layer], feed_dict={input_image: sample_image.reshape(1, 260, 340, 3)})\n",
    "\n",
    "plt.imshow(est_depth[0].reshape(260, 340), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sample_image' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-73478a189d36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_image' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'depth_images' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-58a5ae6d6206>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m49\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_width\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'depth_images' is not defined"
     ]
    }
   ],
   "source": [
    "print(depth_images[49][0:target_height, 0:target_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}